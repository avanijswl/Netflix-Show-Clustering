# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ag49V0GgdCIuKMwA1AZNp_aP263B3Dbx
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy  as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

df = pd.read_csv('netflix_titles.csv')
df = df[['title', 'duration', 'rating', 'listed_in']]
df.dropna(subset=['rating', 'listed_in', 'duration'], inplace=True)

def convert_duration(x):
  if 'min' in x:
    return int (x.replace('min',''))
  elif 'Season' in x:
    return int (x.split()[0]) * 60   #approx season to minutes
  return np.nan

df['duration'] = df['duration'].apply(convert_duration)

# Drop rows where duration couldn't be parsed
df.dropna(subset=['duration'], inplace=True)

#encode genres

df['genre_list'] = df['listed_in'].str.split(', ')
mlb = MultiLabelBinarizer()
genre_encoded = pd.DataFrame(mlb.fit_transform(df['genre_list']),
                             columns = mlb.classes_)

#encode rating

le = LabelEncoder()
df['rating_encoded'] = le.fit_transform(df['rating'])

#final feature set

final_df = pd.concat([genre_encoded, df[['rating_encoded', 'duration']]], axis=1)
final_df = final_df.dropna()

#standardize and apply KMeans

scaler = StandardScaler()
scaled_data = scaler.fit_transform(final_df)

#elbow method to find best K
inertia = []
for k in range(2, 11):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(scaled_data)
    inertia.append(km.inertia_)

plt.plot(range(2, 11), inertia, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()

#final clustering and PCA visualisation
# Choose k=5 for example
kmeans = KMeans(n_clusters=5, random_state=42)

# Ensure df has the same index as final_df before adding the cluster column
df = df.loc[final_df.index]

df['cluster'] = kmeans.fit_predict(scaled_data)

# PCA for 2D plot
pca = PCA(n_components=2)
pca_data = pca.fit_transform(scaled_data)
df['pca1'] = pca_data[:, 0]
df['pca2'] = pca_data[:, 1]

# Scatter plot
plt.figure(figsize=(8,6))
sns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette='Set2')
plt.title('Netflix Show Clusters')
plt.show()

# View cluster counts
df['cluster'].value_counts()

# View cluster counts
df['cluster'].value_counts()

for c in sorted(df['cluster'].unique()):
    print(f"\nCluster {c}:")
    print(df[df['cluster'] == c]['listed_in'].value_counts().head(5))

df.groupby('cluster')[['duration', 'rating_encoded']].mean()

df.to_csv('netflix_clusters.csv', index=False)